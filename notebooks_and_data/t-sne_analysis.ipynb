{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c8720e-70d1-467a-b8a1-e3d601de4912",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df0284-e983-46f5-b0f7-d806b0db46b5",
   "metadata": {},
   "source": [
    "## Load bibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103c1d6-d9b3-4903-b363-2dc302d00615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "#import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65303c-0591-402f-a122-b86268338b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions.functions_tsne_analysis import DataFrameProcessor   # Der Name der Python-Datei ohne die Endung .py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec3ce7-f938-4ced-9792-26336f2d9f00",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cae31-52d0-4bb7-82ae-b88d236e59b7",
   "metadata": {},
   "source": [
    "### Shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba624af-15c6-4689-a982-04e1c29964e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape = pd.read_pickle('data/original_data/pkl/state.pickle')\n",
    "county_shape = pd.read_pickle('data/original_data/pkl/county.pickle')\n",
    "county_shape = county_shape.rename(columns={'GEOID': 'FIPS'})\n",
    "\n",
    "filtered_statefp_north_america = state_shape.loc[~state_shape['NAME'].isin(['Alaska', 'Hawaii', 'Puerto Rico', 'Commonwealth of the Northern Mariana Islands', 'American Samoa', 'United States Virgin Islands', 'Guam']), 'STATEFP']\n",
    "filtered_statefp_alaska = state_shape.loc[state_shape['NAME'].isin(['Alaska']), 'STATEFP']\n",
    "\n",
    "\n",
    "filtered_state_shape_north_america = state_shape[state_shape['STATEFP'].isin(filtered_statefp_north_america)]\n",
    "filtered_state_shape_alaska = state_shape[state_shape['STATEFP'].isin(filtered_statefp_alaska)]\n",
    "\n",
    "\n",
    "filtered_county_shape_north_america = county_shape[county_shape['STATEFP'].isin(filtered_statefp_north_america)]\n",
    "filtered_county_shape_alaska = county_shape[county_shape['STATEFP'].isin(filtered_statefp_alaska)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064f149-b3f5-4ca6-a382-3e2af4fb5822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:04:41.432527Z",
     "iopub.status.busy": "2024-11-25T07:04:41.432290Z",
     "iopub.status.idle": "2024-11-25T07:04:41.435307Z",
     "shell.execute_reply": "2024-11-25T07:04:41.434621Z",
     "shell.execute_reply.started": "2024-11-25T07:04:41.432503Z"
    }
   },
   "source": [
    "### Feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b889a2-76ad-49bf-89f2-65953e04e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_pickle('data/processed_data/pkl/final_df.pickle')\n",
    "print(df_original['FIPS'].nunique())\n",
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fae98f-a009-4c5e-a0ed-dbb710e50eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and copy the columns to drop, preserving them for future use or reference.\n",
    "# Drop the selected columns from the original DataFrame and preprocess the remaining features.\n",
    "columns_to_drop = ['FIPS']\n",
    "df_ids = df_original[columns_to_drop].copy()\n",
    "\n",
    "# Entfernen der Spalten aus dem ursprünglichen DataFrame\n",
    "df_features = df_original.drop(columns=columns_to_drop)\n",
    "df_features, replacement_values, min_values = DataFrameProcessor.replace_nulls_with_small_values(df_features)\n",
    "\n",
    "df_scaled_features = np.log10(df_features)\n",
    "df_original_features = df_original.iloc[:, 1:]\n",
    "\n",
    "df_scaled = pd.concat([df_ids, df_scaled_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3013fa6-985c-4979-a424-e62a621438b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract column names from both original and scaled DataFrames for comparison and visualization.\n",
    "# Create subplots to compare histograms of original and scaled data for each column.\n",
    "\n",
    "columns_original = df_original.columns[1:]\n",
    "columns_scaled = df_scaled.columns[1:]\n",
    "n_columns = len(columns_original)  # Number of columns (assuming both DataFrames have the same number of columns)\n",
    "\n",
    "# Create subplots: 2 columns (one for df_original and one for df_scaled_features)\n",
    "fig, axs = plt.subplots(n_columns, 2, figsize=(12, 4*n_columns))\n",
    "\n",
    "# Plot a histogram for each column in df_original and df_scaled_features\n",
    "for i, col in enumerate(columns_original):\n",
    "    \n",
    "    # Plot histogram for df_original\n",
    "    ax_features = axs[i, 0]\n",
    "    min_val_features = df_original[col].min()\n",
    "    max_val_features = df_original[col].max()\n",
    "    bins_features = np.linspace(min_val_features, max_val_features, 51)\n",
    "    ax_features.hist(df_original[col], bins=bins_features, alpha=0.5, label=col, density=True)\n",
    "    ax_features.set_title(f'Histogram of {col} (df_original)')\n",
    "    ax_features.set_xlabel(col)\n",
    "    ax_features.set_ylabel('Density')\n",
    "    ax_features.legend()\n",
    "\n",
    "    # Plot histogram for df_scaled_features if the column exists\n",
    "    ax_scaled = axs[i, 1]\n",
    "    if col in columns_scaled:\n",
    "        min_val_scaled = df_scaled_features[col].min()\n",
    "        max_val_scaled = df_scaled_features[col].max()\n",
    "        bins_scaled = np.linspace(min_val_scaled, max_val_scaled, 51)\n",
    "        ax_scaled.hist(df_scaled_features[col], bins=bins_scaled, alpha=0.5, label=col, density=True)\n",
    "        ax_scaled.set_title(f'Histogram of {col} (df_scaled)')\n",
    "        ax_scaled.set_xlabel(col)\n",
    "        ax_scaled.set_ylabel('Density')\n",
    "        ax_scaled.legend()\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d07656-b5a9-422b-b8ff-bd0b0a1fdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each feature to a unique identifier in the format \"F1\", \"F2\", etc.\n",
    "# This helps simplify feature naming and improves readability in further processing or analysis.\n",
    "feature_id_dict = {feature: \"F\" + str(i+1) for i, feature in enumerate(df_original_features.columns)}\n",
    "feature_id_dict =  0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9f00f-5367-41fa-a87b-59b108b62ee1",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4365c-ccc4-4fb9-b062-84199f706fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a t-SNE model with specific parameters for dimensionality reduction.\n",
    "# This will be applied to the unfiltered dataset to project high-dimensional features into a 2D space for visualization.\n",
    "tsne_3 = TSNE(n_components=2, perplexity=40, learning_rate=200, n_iter=500, random_state=42)\n",
    "X_tsne_3 = tsne_3.fit_transform(df_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63bf28d-ac97-4ba5-ba71-e40f83cd6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to visualize the t-SNE results for the third dataset.\n",
    "# The plot displays the two t-SNE components on the x- and y-axes, providing insight into the structure of the data.\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter_3 = plt.scatter(X_tsne_3[:, 0], X_tsne_3[:, 1], s=5)\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "\n",
    "# Adjust layout for better spacing and display the plot.\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b1409-2574-4fc7-a9cb-2627cb31486c",
   "metadata": {},
   "source": [
    "## Cluster identification\n",
    "\n",
    "This process involves reducing the dimensionality of a high-dimensional dataset using t-SNE and then clustering the data with DBSCAN. \n",
    "The t-SNE step transforms complex data into two dimensions, capturing key structure and relationships in a way that’s visually interpretable. \n",
    "**Afterward, DBSCAN identifies clusters based on density, distinguishing between core clusters and noise points.** \n",
    "The resulting clusters are then visualized in a scatter plot, with color-coded points representing different clusters and noise, allowing for easy interpretation of the data's underlying patterns and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147224fa-ff1c-48b6-96ac-27f2a9e4e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for the grid search\n",
    "param_grid = {\n",
    "    'eps': np.arange(2.0, 4.1, 0.5),      # Example range for eps values\n",
    "    'min_samples': np.arange(20, 36, 5), # Example range for min_samples values\n",
    "}\n",
    "\n",
    "# List to store the results of the grid search\n",
    "grid_results = []\n",
    "\n",
    "# Iterate through all parameter combinations in the grid search\n",
    "for params in ParameterGrid(param_grid):\n",
    "    # Initialize DBSCAN with the current parameters\n",
    "    dbscan = DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "    cluster_labels = dbscan.fit_predict(X_tsne_3)\n",
    "\n",
    "    # Count clusters and noise points\n",
    "    cluster_counts = Counter(cluster_labels)\n",
    "    total_points = len(cluster_labels)\n",
    "    noise_count = cluster_counts[-1] if -1 in cluster_counts else 0\n",
    "    noise_ratio = noise_count / total_points\n",
    "\n",
    "    # Check if more than one cluster was found and noise is below 10%\n",
    "    if len(set(cluster_labels)) > 1 and noise_ratio <= 0.1:\n",
    "        # Compute the silhouette score\n",
    "        silhouette_avg = silhouette_score(X_tsne_3, cluster_labels)\n",
    "        \n",
    "        # Compute silhouette coefficients for each point\n",
    "        silhouette_vals = silhouette_samples(X_tsne_3, cluster_labels)\n",
    "        \n",
    "        # Count points with negative silhouette coefficients\n",
    "        negative_silhouette_count = np.sum(silhouette_vals < 0)\n",
    "\n",
    "        # Save the result\n",
    "        grid_results.append({\n",
    "            'eps': params['eps'],\n",
    "            'min_samples': params['min_samples'],\n",
    "            'silhouette_avg': silhouette_avg,\n",
    "            'negative_silhouette_count': negative_silhouette_count,\n",
    "            'noise_ratio': noise_ratio\n",
    "        })\n",
    "\n",
    "        # Silhouette coefficients for each cluster\n",
    "        unique_labels = set(cluster_labels)\n",
    "        cluster_silhouette = {label: np.mean(silhouette_vals[cluster_labels == label]) \n",
    "                              for label in unique_labels if label != -1}\n",
    "\n",
    "        # Define colors for clusters\n",
    "        colors = plt.cm.Paired(np.linspace(0, 1, len(unique_labels) + 1))  # +1 for noise\n",
    "        color_dict = {label: \"#{:02x}{:02x}{:02x}\".format(int(c[0]*255), int(c[1]*255), int(c[2]*255)) \n",
    "                      for label, c in zip(unique_labels, colors)}\n",
    "        color_dict[-1] = \"#000000\"  # Black for noise\n",
    "\n",
    "        # Create figure with two side-by-side plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Create t-SNE scatter plot with cluster coloring\n",
    "        for label in unique_labels:\n",
    "            if label == -1:\n",
    "                ax1.scatter(X_tsne_3[cluster_labels == label, 0], X_tsne_3[cluster_labels == label, 1],\n",
    "                            color=color_dict[label], label=f'Noise ({cluster_counts[label]})', alpha=0.6, s=4)\n",
    "            else:\n",
    "                ax1.scatter(X_tsne_3[cluster_labels == label, 0], X_tsne_3[cluster_labels == label, 1],\n",
    "                            color=color_dict[label], label=f'Cluster {label} ({cluster_counts[label]})', alpha=0.6, s=4)\n",
    "\n",
    "        ax1.set_title(f't-SNE with DBSCAN Clustering\\n(eps={params[\"eps\"]}, min_samples={params[\"min_samples\"]})')\n",
    "        ax1.set_xlabel('t-SNE 1')\n",
    "        ax1.set_ylabel('t-SNE 2')\n",
    "        ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "\n",
    "        # Silhouette visualization for clusters\n",
    "        y_lower = 10\n",
    "        for cluster in unique_labels:\n",
    "            if cluster != -1:\n",
    "                cluster_vals = silhouette_vals[cluster_labels == cluster]\n",
    "                cluster_vals.sort()\n",
    "                size_cluster = cluster_vals.shape[0]\n",
    "                ax2.fill_betweenx(np.arange(y_lower, y_lower + size_cluster),\n",
    "                                  0, cluster_vals, color=color_dict[cluster])\n",
    "                ax2.text(-0.05, y_lower + size_cluster / 2, str(cluster))\n",
    "                y_lower += size_cluster\n",
    "\n",
    "        ax2.set_title(\"Silhouette Plot for All Clusters\")\n",
    "        ax2.set_xlabel(\"Silhouette Coefficient\")\n",
    "        ax2.set_ylabel(\"Cluster\")\n",
    "        ax2.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print silhouette results\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Parameters: eps={params['eps']}, min_samples={params['min_samples']}\")\n",
    "        print(f\"Average Silhouette Coefficient: {silhouette_avg}\")\n",
    "        print(f\"Number of Points with Negative Silhouette Coefficients: {negative_silhouette_count}\")\n",
    "        print(f\"Number of Noise Points: {noise_count}\")\n",
    "        print(f\"Noise Ratio: {noise_ratio:.2%}\")\n",
    "        for cluster, score in cluster_silhouette.items():\n",
    "            print(f\"Average Silhouette Coefficient for Cluster {cluster}: {score:.3f}\")\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "# Total number of points and noise points\n",
    "total_points = len(grid_results)  # Total number of points\n",
    "noise_threshold = 0.05  # 5% threshold for noise\n",
    "\n",
    "# Find the best result\n",
    "best_result = max(\n",
    "    grid_results,\n",
    "    key=lambda x: (\n",
    "        x['silhouette_avg'],\n",
    "        -x['negative_silhouette_count'] if (x['negative_silhouette_count'] / total_points) < noise_threshold else float('-inf')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Print the best result\n",
    "print(\"Best Parameter Combination:\")\n",
    "print(f\"eps: {best_result['eps']}, min_samples: {best_result['min_samples']}\")\n",
    "print(f\"Average Silhouette Coefficient: {best_result['silhouette_avg']}\")\n",
    "print(f\"Number of Points with Negative Silhouette Coefficients: {best_result['negative_silhouette_count']}\")\n",
    "print(f\"Noise Ratio: {best_result['noise_ratio']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa27a09-690f-4ed1-97fa-431afed03e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit DBSCAN clusterer\n",
    "dbscan = DBSCAN(eps=3, min_samples=25)\n",
    "cluster_labels = dbscan.fit_predict(X_tsne_3)\n",
    "cluster_counts = Counter(cluster_labels)\n",
    "\n",
    "\n",
    "# Create color assignments\n",
    "unique_labels = np.unique(cluster_labels)\n",
    "colors = plt.cm.Dark2(np.linspace(0, 1, len(unique_labels) + 1))  # +1 for noise\n",
    "\n",
    "# Dictionary to map colors to cluster IDs using color strings\n",
    "color_dict = {label: \"#{:02x}{:02x}{:02x}\".format(int(c[0]*255), int(c[1]*255), int(c[2]*255)) for i, (label, c) in enumerate(zip(unique_labels, colors))}\n",
    "color_dict[-1] = \"#000000\"  # Black for \"Noise\"\n",
    "\n",
    "# Add a new column for colors to the DataFrame\n",
    "df_original['cluster_id'] = cluster_labels\n",
    "df_scaled['cluster_id'] = cluster_labels\n",
    "\n",
    "df_original['color'] = df_original['cluster_id'].map(color_dict)\n",
    "df_scaled['color'] = df_scaled['cluster_id'].map(color_dict)\n",
    "\n",
    "\n",
    "# Count the number of \"Noise\" points\n",
    "noise_count = cluster_counts[-1]\n",
    "print(f'Number of \"Noise\" points: {noise_count}')\n",
    "\n",
    "# Plot for the third dataset with clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot with cluster colors\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        # -1 represents noise, so plot in black\n",
    "        plt.scatter(X_tsne_3[cluster_labels == label, 0], X_tsne_3[cluster_labels == label, 1],\n",
    "                    color=color_dict[label], label=f'Noise ({noise_count})', alpha=0.6, s=5)\n",
    "    else:\n",
    "        plt.scatter(X_tsne_3[cluster_labels == label, 0], X_tsne_3[cluster_labels == label, 1],\n",
    "                    color=color_dict[label], label=f'Cluster {label} ({cluster_counts[label]})', alpha=0.6, s=5)\n",
    "\n",
    "plt.title('t-SNE Visualization with DBSCAN Clustering')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "\n",
    "# Add legend and position it below the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0735-40ff-406e-800d-fd78da574b4a",
   "metadata": {},
   "source": [
    "## t-SNE with heatmap for every single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fab004-51ed-46ad-b9e3-f8f3e304c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel DataFrame (X_unfiltered_copy) erstellen\n",
    "# X_unfiltered_copy = pd.DataFrame(...) # Dein DataFrame hier\n",
    "\n",
    "\n",
    "# Iteriere über die Spalten, um Heatmaps zu erstellen\n",
    "for col in df_scaled_features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    scatter = plt.scatter(X_tsne_3[:, 0], X_tsne_3[:, 1], c=df_scaled_features[col], cmap='viridis', alpha=0.7, s =5)\n",
    "    \n",
    "    # Farbskala hinzufügen\n",
    "    plt.colorbar(scatter, label=col)\n",
    "    plt.title(f't-SNE Visualisierung für {col}')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "\n",
    "    # Layout anpassen und anzeigen\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592efb4-836a-4f63-adc3-a95c85081ff8",
   "metadata": {},
   "source": [
    "## Cluster map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a69bf-307f-423f-97ef-9a99c445c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_county_shape_north_america_merged = filtered_county_shape_north_america.merge(df_scaled[['FIPS', 'cluster_id','color']], on='FIPS', how='left')\n",
    "filtered_county_shape_north_america_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8960dd5-edd2-4afa-b6f1-b49cd225dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_county_shape_alaska_merged = filtered_county_shape_alaska.merge(df_scaled[['FIPS', 'cluster_id','color']], on='FIPS', how='left')\n",
    "filtered_county_shape_alaska_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69665421-ab10-4f5b-a006-1ecc33c5476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot.\n",
    "fig, ax = plt.subplots(figsize=(15, 7))  # Initialize a figure and axes for the plot.\n",
    "filtered_county_shape_north_america_merged.plot(facecolor=filtered_county_shape_north_america_merged['color'], linewidth=0.8, ax=ax, edgecolor='0.8', legend=False)  # Plot counties with assigned colors.\n",
    "\n",
    "# Overlay county and state outlines\n",
    "filtered_county_shape_north_america.plot(ax=ax, color='none', edgecolor='gray', linewidth=0.4, alpha=0.5)  # Overlay county outlines.\n",
    "filtered_state_shape_north_america.plot(ax=ax, color='none', edgecolor='black', linewidth=0.5)  # Overlay state outlines.\n",
    "\n",
    "# Create a custom legend for the clusters.\n",
    "unique_clusters = sorted(filtered_county_shape_north_america_merged['cluster_id'].unique())  # Get unique cluster IDs\n",
    "cluster_counts = filtered_county_shape_north_america_merged['cluster_id'].value_counts()  # Count the number of entries per cluster\n",
    "legend_elements = []\n",
    "\n",
    "# Loop through each cluster and create legend elements\n",
    "for cluster in unique_clusters:\n",
    "    if cluster == -1:\n",
    "        # For cluster -1, use gray color and label as 'noise'\n",
    "        count = cluster_counts.get(-1, 0)\n",
    "        legend_elements.append(Patch(facecolor='black', label=f'noise ({count})'))\n",
    "    else:\n",
    "        color = filtered_county_shape_north_america_merged.loc[filtered_county_shape_north_america_merged['cluster_id'] == cluster, 'color'].values[0]\n",
    "        count = cluster_counts.get(cluster, 0)\n",
    "        legend_elements.append(Patch(facecolor=color, label=f'{cluster} ({count})'))  # Add count to the legend label\n",
    "\n",
    "# Add the legend to the plot.\n",
    "ax.legend(handles=legend_elements, title='Cluster ID', loc='lower right')  # Position the legend in the lower right corner.\n",
    "\n",
    "plt.title('Cluster Assignment to Counties')  # Set the plot title.\n",
    "plt.axis('off')  # Turn off the axes for a cleaner look.\n",
    "plt.show()  # Display the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ad717-eb00-4bca-837b-93206e30ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific grid layout\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[65, 35])  # Set width ratios for subplots\n",
    "\n",
    "# Create subplots using the grid spec\n",
    "ax1 = fig.add_subplot(gs[0])  # North America subplot\n",
    "ax2 = fig.add_subplot(gs[1])  # Alaska subplot\n",
    "\n",
    "# Plot for North America\n",
    "filtered_county_shape_north_america_merged.plot(\n",
    "    facecolor=filtered_county_shape_north_america_merged['color'], \n",
    "    linewidth=0.8, \n",
    "    ax=ax1, \n",
    "    edgecolor='0.8', \n",
    "    legend=False\n",
    ")  # Plot counties with assigned colors.\n",
    "\n",
    "# Overlay county and state outlines for North America\n",
    "filtered_county_shape_north_america.plot(\n",
    "    ax=ax1, \n",
    "    color='none', \n",
    "    edgecolor='gray', \n",
    "    linewidth=0.4, \n",
    "    alpha=0.5\n",
    ")  # Overlay county outlines.\n",
    "filtered_state_shape_north_america.plot(\n",
    "    ax=ax1, \n",
    "    color='none', \n",
    "    edgecolor='black', \n",
    "    linewidth=0.5\n",
    ")  # Overlay state outlines.\n",
    "\n",
    "# Create a custom legend for the clusters\n",
    "unique_clusters = sorted(filtered_county_shape_north_america_merged['cluster_id'].unique())  # Get unique cluster IDs\n",
    "cluster_counts = filtered_county_shape_north_america_merged['cluster_id'].value_counts()  # Count the number of entries per cluster\n",
    "legend_elements = []\n",
    "\n",
    "# Loop through each cluster and create legend elements\n",
    "for cluster in unique_clusters:\n",
    "    if cluster == -1:\n",
    "        # For cluster -1, use gray color and label as 'noise'\n",
    "        count = cluster_counts.get(-1, 0)\n",
    "        legend_elements.append(Patch(facecolor='black', label=f'noise ({count})'))\n",
    "    else:\n",
    "        color = filtered_county_shape_north_america_merged.loc[filtered_county_shape_north_america_merged['cluster_id'] == cluster, 'color'].values[0]\n",
    "        count = cluster_counts.get(cluster, 0)\n",
    "        legend_elements.append(Patch(facecolor=color, label=f'{cluster} ({count})'))  # Add count to the legend label\n",
    "\n",
    "# Add the legend to the plot\n",
    "ax1.legend(handles=legend_elements, title='Cluster ID', loc='lower right')  # Position the legend in the lower right corner.\n",
    "ax1.set_title('North America County Clusters')\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "\n",
    "# Plot for Alaska\n",
    "# Assuming filtered_county_shape_alaska has a 'color' column based on clusters\n",
    "filtered_county_shape_alaska_merged.plot(\n",
    "    facecolor=filtered_county_shape_alaska_merged['color'], \n",
    "    linewidth=0.5, \n",
    "    ax=ax2, \n",
    "    edgecolor='black'\n",
    ")  # Plot counties in Alaska with assigned colors.\n",
    "\n",
    "# Overlay state outlines for Alaska\n",
    "filtered_county_shape_alaska_merged.plot(\n",
    "    ax=ax2, \n",
    "    color='none', \n",
    "    edgecolor='black', \n",
    "    linewidth=0.5\n",
    ")  # Overlay state outlines for Alaska.\n",
    "\n",
    "# Set limits and labels for Alaska\n",
    "ax2.set_xlim(-180, -130)\n",
    "ax2.set_ylim(50, 72)\n",
    "ax2.set_title('Alaska County Clusters')\n",
    "ax2.set_xlabel('Longitude')\n",
    "ax2.set_ylabel('Latitude')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a433b0-3da1-482a-98f9-bce51b6328b6",
   "metadata": {},
   "source": [
    "## Feature characteristics for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e412890-32dc-47bc-8e01-10f8c5254072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns that are not 'cluster_id', 'color', 'FIPS'\n",
    "numeric_columns = [col for col in df_scaled.columns if col not in ['cluster_id', 'color', 'FIPS']]\n",
    "\n",
    "# Calculate the mean and standard deviation for each cluster\n",
    "mean_values = df_scaled.groupby('cluster_id')[numeric_columns].mean()\n",
    "std_values = df_scaled.groupby('cluster_id')[numeric_columns].std()  # Calculate standard deviation\n",
    "count_values = df_scaled.groupby('cluster_id')[numeric_columns].count()  # Count values per cluster\n",
    "standard_error = std_values / np.sqrt(count_values)  # Calculate standard error\n",
    "\n",
    "# Calculate the overall mean for each cluster and sort clusters accordingly\n",
    "mean_values['overall_mean'] = mean_values.mean(axis=1)\n",
    "mean_values = mean_values.sort_values(by='overall_mean', ascending=False)\n",
    "\n",
    "# Plot preparation\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Color selection based on the 'color' column\n",
    "colors = df_scaled[['cluster_id', 'color']].drop_duplicates().set_index('cluster_id')['color']\n",
    "\n",
    "# Create the line plot with the corresponding colors\n",
    "for cluster in mean_values.index:\n",
    "    # Get the color for the current cluster\n",
    "    color = colors.get(cluster, 'gray')  # Use the color or default to 'gray' if no color is found\n",
    "    \n",
    "    # Plot the mean values\n",
    "    plt.plot(mean_values.columns[:-1], mean_values.loc[cluster, mean_values.columns[:-1]], marker='o', \n",
    "             label=f'Cluster {cluster}', color=color)\n",
    "    \n",
    "    # Add the confidence interval (e.g., 95% CI)\n",
    "    ci_upper = mean_values.loc[cluster, mean_values.columns[:-1]] + 1.96 * standard_error.loc[cluster]\n",
    "    ci_lower = mean_values.loc[cluster, mean_values.columns[:-1]] - 1.96 * standard_error.loc[cluster]\n",
    "    \n",
    "    # Check dimensions\n",
    "    if ci_upper.shape[0] == mean_values.columns[:-1].shape[0]:\n",
    "        # Fill the area for the confidence interval\n",
    "        plt.fill_between(mean_values.columns[:-1], ci_lower, ci_upper, color=color, alpha=0.2)\n",
    "    else:\n",
    "        print(f\"Dimension mismatch for cluster {cluster}: ci_upper {ci_upper.shape}, mean_values {mean_values.columns[:-1].shape}\")\n",
    "\n",
    "# Adjust the plot\n",
    "plt.title('Mean Values of Features by Cluster with Confidence Intervals (Ranked by Overall Mean)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xticks(rotation=90)  # For better readability of X-axis labels\n",
    "# Add legend and place it below the plot\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1), ncol=3)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Output the clusters in descending order of their overall mean\n",
    "print(\"Cluster Ranking by Overall Mean:\")\n",
    "print(mean_values[['overall_mean']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856acf6e-db5e-4305-a6de-e37e4288ebc8",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "The code trains a separate Random Forest model for each unique cluster to assess the importance of various features in distinguishing each cluster.  \n",
    "The results are visualized in a heatmap, displaying the significance of each feature across different clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a27132-885b-44b2-90df-fef22e885132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns 'cluster_id', 'color', 'FIPS'\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "X = df_scaled[df_scaled.columns[1:-2]]\n",
    "clusters = df_scaled['cluster_id'].unique()\n",
    "\n",
    "# Dictionary to store feature importances for each cluster\n",
    "feature_importances = []\n",
    "\n",
    "# Iterate over each cluster\n",
    "for cluster in clusters:\n",
    "    # Create the target variable: 1 for the current cluster, 0 for all others\n",
    "    y = (df_scaled['cluster_id'] == cluster).astype(int)\n",
    "    \n",
    "    # Train a Random Forest for the binary problem\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Store the results in the list as a DataFrame\n",
    "    for feature, importance in zip(X.columns, importances):\n",
    "        feature_importances.append({'Cluster': cluster, 'Feature': feature, 'Importance': round(importance, 3)})\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "importance_df = pd.DataFrame(feature_importances)\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "pivot = importance_df.pivot(index=\"Feature\", columns=\"Cluster\", values=\"Importance\")\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot, cmap=\"YlGnBu\", annot=True, cbar_kws={'label': 'Feature Importance'}, \n",
    "            fmt=\".3f\", annot_kws={\"size\": 9})  # fmt for decimals and annot_kws for smaller font size\n",
    "plt.title('Feature Importance Heatmap by Cluster', fontsize=12)\n",
    "plt.xlabel('Cluster ID', fontsize=10)\n",
    "plt.ylabel('Features', fontsize=10)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc07d48-67e4-4156-a496-e8f662c443ba",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26202aa9-90e7-4121-81cf-02a5165d22e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = filtered_county_shape_north_america_merged[['FIPS', 'cluster_id', 'color']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423174d6-b19e-43ac-9eae-847066951ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns 'cluster_id', 'color', 'FIPS'\n",
    "X = df_scaled[df_scaled.columns[1:-2]]\n",
    "clusters = df_scaled['cluster_id'].unique()\n",
    "\n",
    "# Dictionary to store feature importances for each cluster\n",
    "feature_importances = []\n",
    "\n",
    "# Iterate over each cluster and calculate feature importances\n",
    "for cluster in clusters:\n",
    "    y = (df_scaled['cluster_id'] == cluster).astype(int)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    for feature, importance in zip(X.columns, importances):\n",
    "        feature_importances.append({'Cluster': cluster, 'Feature': feature, 'Importance': round(importance, 3)})\n",
    "\n",
    "importance_df = pd.DataFrame(feature_importances)\n",
    "\n",
    "# Extract the top-3 features for each cluster\n",
    "top_features_by_cluster = (\n",
    "    importance_df.groupby(\"Cluster\")\n",
    "    .apply(lambda x: x.nlargest(5, 'Importance'))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Loop through clusters for scatter, map, and t-SNE with textbox for top-3 features\n",
    "for cluster in clusters:\n",
    "    cluster_data = filtered_county_shape_north_america_merged[filtered_county_shape_north_america_merged['cluster_id'] == cluster]\n",
    "    cluster_shape_data = filtered_county_shape_north_america_merged[filtered_county_shape_north_america_merged['cluster_id'] == cluster]\n",
    "\n",
    "    fig, (ax2, ax3) = plt.subplots(1, 2, figsize=(20, 7), gridspec_kw={'width_ratios': [6, 4]})\n",
    "    \n",
    "    # Map plot\n",
    "    cluster_shape_data.plot(facecolor=cluster_shape_data['color'], linewidth=0.8, ax=ax2, edgecolor='0.8', legend=False)\n",
    "    filtered_county_shape_north_america.plot(ax=ax2, color='none', edgecolor='gray', linewidth=0.4, alpha=0.5)\n",
    "    filtered_state_shape_north_america.plot(ax=ax2, color='none', edgecolor='black', linewidth=0.5)\n",
    "    count = cluster_counts.get(cluster, 0)\n",
    "    if cluster == -1:\n",
    "        legend_elements = [Patch(facecolor='gray', label=f'Noise ({count})')]\n",
    "    else:\n",
    "        color = cluster_shape_data['color'].values[0]\n",
    "        legend_elements = [Patch(facecolor=color, label=f'Cluster {cluster} ({count})')]\n",
    "    ax2.legend(handles=legend_elements, title='Cluster ID', loc='lower left')\n",
    "    ax2.set_title(f'Cluster {cluster} Assignment to Counties')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Add Top-3 Features Textbox\n",
    "    top_features = top_features_by_cluster[top_features_by_cluster['Cluster'] == cluster]\n",
    "    feature_text = '\\n'.join([f\"{row['Feature']}: {row['Importance']}\" for _, row in top_features.iterrows()])\n",
    "    ax2.text(0.95, 0.35, f\"Top Features:\\n{feature_text}\", transform=ax2.transAxes, ha=\"center\", va=\"top\",\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"white\"))\n",
    "\n",
    "    # t-SNE plot for cluster\n",
    "    for label in clusters:\n",
    "        if label == cluster:\n",
    "            ax3.scatter(X_tsne_3[cluster_labels == label, 0], X_tsne_3[cluster_labels == label, 1], \n",
    "                        color=color_dict[label], label=f'Cluster {label} ({cluster_counts[label]})', alpha=0.75, s=10)\n",
    "        else:\n",
    "            ax3.scatter(X_tsne_3[cluster_labels == label, 0], X_tsne_3[cluster_labels == label, 1], \n",
    "                        color=\"gray\", alpha=0.2, s=10)\n",
    "    \n",
    "    ax3.set_title(f't-SNE Visualization - Cluster {cluster} Highlighted')\n",
    "    ax3.set_xlabel('t-SNE 1')\n",
    "    ax3.set_ylabel('t-SNE 2')\n",
    "    ax3.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636227d-5b1e-4a74-93b6-5b898d092937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-DM+TF-Klon",
   "language": "python",
   "name": "py3.10-dm-tf-cloned"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
